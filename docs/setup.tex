\chapter{Setting up Swifty}
\label{ch:setup}

\section{Components}
\label{sec:setup-components}

Swifty is a set of distributed components:

\begin{itemize}
\item{\code{swifty} daemon which serves incoming HTTP requests and may
	be considered as a gate to all underlied serivces;}
\item{\code{swy-wdog} daemon which runs inside each container serving as a watchdog;}
\item{Kubernetes which manage resources requested by \code{swifty};}
\item{Docker which spawns and deletes containers;}
\item{NFS server and client to access functions source code
	to run inside containers;}
\item{Mongo database used by \code{swifty} to track resources;}
\item{Middleware programs (MariaDB, RabbitMQ and etc) to serve requests from
	containers.}
\end{itemize}

Each of these components may preset on sole hardware node or may be
completely distributed in one per node mode.

To provide an example of the setup procedure we create two virtual machines
\textbf{fc1} and \textbf{fc2} based on
\urlref{https://getfedora.org/en/workstation/download/}{Fedora 26} distributive
with addresses \code{192.168.122.68} and \code{192.168.122.102} respectively.
Where \textbf{fc1} is a \emph{master} node and \textbf{fc2} is a \emph{slave}.

Thus \emph{master} will carry Kubernetes, Keystone, MongoDB, NFS server
and \code{swifty}, while \textbf{fc2} for NFS client and Docker containers
with \code{swy-wdog} watchdog inside.

Add the following records to \code{/etc/hosts} on both nodes

\begin{lstlisting}
192.168.122.68		fed-master
192.168.122.68		fc1
192.168.122.102		fed-node
192.168.122.102		fc2
\end{lstlisting}

Make sure they are connected and reacheable from each other.

Disable the firewall

\begin{lstlisting}
systemctl stop firewalld
systemctl disable firewalld
\end{lstlisting}

and SElinux if present in \code{/etc/sysconfig/selinux}

\begin{lstlisting}
SELINUX=disabled
\end{lstlisting}

Reboot the machines.

\section{Setting up Keystone}
\label{sec:setup-keystone}

Follow the
\urlref{https://docs.openstack.org/keystone/latest/install/index.html}{instructions}
to install \urlref{https://www.openstack.org/software/releases/ocata/components/keystone}
{Pike} keystone.

We install it the \emph{master} node.

\begin{lstlisting}
dnf -y install mariadb mariadb-server-utils
systemctl start mariadb
systemctl enable mariadb
\end{lstlisting}

Configure \code{mariadb} and add a database for the keystone.

\begin{lstlisting}
mysqladmin -u root password aiNe1sah9ichu1re
mysql -u root -p
CREATE DATABASE keystone;
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' \
IDENTIFIED BY 'Ja7hey6keiThoyoi';
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' \
IDENTIFIED BY 'Ja7hey6keiThoyoi';
\end{lstlisting}

Install the packages needed (if shortener failed simply search for
\code{rdo-release-pike.rpm}).

\begin{lstlisting}
dnf install https://goo.gl/qsXjTf
dnf -y install openstack-keystone httpd mod_wsgi python-openstackclient
\end{lstlisting}

Configure \code{/etc/keystone/keystone.conf}

\begin{lstlisting}
[database]
...
connection = mysql+pymysql://keystone:Ja7hey6keiThoyoi@controller/keystone

[token]
...
provider = fernet
\end{lstlisting}

Prepare keystone

\begin{lstlisting}
/bin/sh -c "keystone-manage db_sync" keystone
keystone-manage fernet_setup --keystone-user \
	keystone --keystone-group keystone
keystone-manage credential_setup --keystone-user \
	keystone --keystone-group keystone

keystone-manage bootstrap --bootstrap-password Cae6ThiekuShiece \
  --bootstrap-admin-url http://controller:35357/v3/ \
  --bootstrap-internal-url http://controller:5000/v3/ \
  --bootstrap-public-url http://controller:5000/v3/ \
  --bootstrap-region-id RegionOne
\end{lstlisting}

In \code{/etc/httpd/conf/httpd.conf} setup server name

\begin{lstlisting}
ServerName controller
\end{lstlisting}

and add this alias to \code{/etc/hosts}

\begin{lstlisting}
127.0.0.1               controller
\end{lstlisting}

Link the wsgi config and start web server

\begin{lstlisting}
ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/
systemctl enable httpd.service
systemctl start httpd.service
\end{lstlisting}

Put the folling variables into \code{\textasciitilde/.bashrc}

\begin{lstlisting}
export OS_USERNAME=admin
export OS_PASSWORD=Cae6ThiekuShiece
export OS_PROJECT_NAME=admin
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_DOMAIN_NAME=Default
export OS_AUTH_URL=http://controller:5000/v3
export OS_IDENTITY_API_VERSION=3
\end{lstlisting}

Now setup a role, a user and a project. There are three roles
used by swifty. The swifty.admin is allowed to manage users,
swifty.owner is allowed to work with gate on his project and
manage himself, and swifty.ui is allowed to create new users.

All users and projects should (but not must) live in a separate
domain, as swy-admd lists all users in it.

\begin{lstlisting}
openstack role create swifty.admin
openstack role create swifty.owner
openstack role create swifty.ui
openstack domain create swifty
openstack project create --domain swifty swyadmin
openstack user create --project swyadmin --domain swifty --password 1q2w3e swyadmin
openstack role add --user-domain swifty --user swyadmin --project-domain swifty --project swyadmin swifty.admin
openstack role add --user-domain swifty --user swyadmin --project-domain swifty --project swyadmin swifty.owner
\end{lstlisting}

Now the swyadmin is registered as both, admin and owner.

The \code{keystone} section in gate config must match the values
provided here (see \fullref{subsec:setup-swifty-conf}).

\section{Setting up IPVS}
\label{sec:setup-nfs}

To balance a traffic between pods
\urlref{http://www.linuxvirtualserver.org/software/ipvs.html}{IPVS} is used.

First install \code{ipvsadm} utility on the \emph{master} node

\begin{lstlisting}
dnf -y install ipvsadm
\end{lstlisting}

Then add a route entry so that the kernel would push traffic
frames into the interface where all other pods will be able
to receive. In this manual it is \code{flannel.1} device.

\begin{lstlisting}
ip r add 10.8.0.0/24 dev flannel.1
\end{lstlisting}

The address of the interface must be in a local network and
should match \code{balancer} section in configuratio file
(see \fullref{subsec:setup-swifty-conf}).

When \code{swifty} handles the function it basically takes new
address from the config pool and adds pathes with \code{ipvsadm}
help.

Just for example here is a round robin balancing added for
testing function with four replicas.

\begin{lstlisting}
[root@fc3 test]# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.8.0.2:8687 rr
  -> 16.16.42.2:8687              Masq    1      0          0         
  -> 16.16.42.4:8687              Masq    1      0          0         
  -> 16.16.103.2:8687             Masq    1      0          0         
  -> 16.16.103.3:8687             Masq    1      0          0         
\end{lstlisting}

\section{Setting up NFS}
\label{sec:setup-nfs}

Setup NFS on both nodes

\begin{lstlisting}
dnf -y install nfs-utils
systemctl start nfs
systemctl enable nfs
\end{lstlisting}

Configure the \emph{master}

\begin{lstlisting}
mkdir -p /home/swifty-volume
echo "/home/swifty-volume fed-node(rw,sync,no_root_squash)" >> /etc/exports
exportfs -a
\end{lstlisting}

Configure the \emph{slave}

\begin{lstlisting}
mkdir -p /home/swifty-volume
echo "fed-master:/home/swifty-volume /home/swifty-volume nfs user,rw,exec 0 0" >> /etc/fstab
mount /home/swifty-volume
\end{lstlisting}

\section{Setting up Docker}
\label{sec:setup-docker}

On the \emph{slave} node install \urlref{https://www.docker.com/}{Docker}.

\begin{lstlisting}
dnf -y install docker
systemctl enable docker
\end{lstlisting}

Then add and configure a separate disk for the docker images
(say separate virtual qcow2 disk with 20G in size attached to
the virtual machine, in our example it is \code{/dev/sdb}).

\begin{lstlisting}
systemctl stop docker
rm -rf /var/lib/docker
pvcreate /dev/sdb
vgcreate docker /dev/sdb
lvcreate --wipesignatures y -n thinpool docker -l 95%VG
lvcreate --wipesignatures y -n thinpoolmeta docker -l 1%VG
lvconvert -y \
--zero n \
-c 512K \
--thinpool docker/thinpool \
--poolmetadata docker/thinpoolmeta
\end{lstlisting}

Then in \code{/etc/lvm/profile/docker-thinpool.profile} add

\begin{lstlisting}
activation {
  thin_pool_autoextend_threshold=80
  thin_pool_autoextend_percent=20
}
\end{lstlisting}

and run

\begin{lstlisting}
lvchange --metadataprofile docker-thinpool docker/thinpool
\end{lstlisting}

In \code{/etc/docker/daemon.json} put

\begin{lstlisting}
{
    "storage-driver": "devicemapper",
    "storage-opts": [
    "dm.thinpooldev=/dev/mapper/docker-thinpool",
    "dm.use_deferred_removal=true",
    "dm.use_deferred_deletion=true"
    ]
}
\end{lstlisting}

and finally run the docker itself

\begin{lstlisting}
systemctl start docker
\end{lstlisting}

\section{Setting up Ceph}
\label{sec:setup-ceph}

% To add HDD with prlctl
% {0296fd67-f004-4eb8-9de7-a5997a4e4d98}  running      -               VM crvm0.kt
% {488b10ac-0280-49be-9856-6ad21a4c6e59}  running      -               VM crvm1.kt
% {75fa250a-5e02-4222-855a-2ef9f54754a4}  running      -               VM crvm2.kt
% {ac0bf901-d9b9-4e5e-bb19-a5b4e7a65d18}  running      -               VM crvm3.kt
% {3f5d6d08-b7d5-42ab-b0d2-b53fe4834d9a}  running      -               VM crvm4.kt
% {4e893be5-af4c-4503-97c5-d43a62008379}  running      -               VM crvm5.kt
% prlctl set 0296fd67-f004-4eb8-9de7-a5997a4e4d98 --device-add hdd --size 20000
% prlctl set 75fa250a-5e02-4222-855a-2ef9f54754a4 --device-add hdd --size 20000
% prlctl set 488b10ac-0280-49be-9856-6ad21a4c6e59 --device-add hdd --size 20000
% prlctl set ac0bf901-d9b9-4e5e-bb19-a5b4e7a65d18 --device-add hdd --size 20000
% prlctl set 3f5d6d08-b7d5-42ab-b0d2-b53fe4834d9a --device-add hdd --size 20000
% prlctl set 4e893be5-af4c-4503-97c5-d43a62008379 --device-add hdd --size 20000

%[ceph@crvm3 ceph-admin]$ cat /etc/hosts
%127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
%::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
%10.94.96.216            crvm0
%10.94.96.217            crvm1
%10.94.96.218            crvm2
%10.94.96.220            crvm3
%10.94.96.219            crvm4
%10.94.96.221            crvm5

To store S3 objects the \urlref{http://ceph.com/}{Ceph} backed in used.
It is managed by \code{crvm3} admin node which governs three slave nodes
(hosts \code{crvm3}, \code{crvm4}, \code{crmv5}).

Three nodes are the minimum, note we reuse admin node as one of slave node
for example case. In production it should rather be a separate host as
ceph documentation implies.

On admin node install

\begin{lstlisting}
dnf -y install ceph-deploy
\end{lstlisting}

Run the following preliminary actions on all nodes

\begin{lstlisting}
useradd -d /home/ceph -m ceph
passwd ceph
echo "ceph ALL = (root) NOPASSWD:ALL" > /etc/sudoers.d/ceph
chmod 0440 /etc/sudoers.d/ceph
chown -R ceph:users /home/ceph/
\end{lstlisting}

For password the value \code{Ekeececae1fee6Da} is used. Then
new keys are generated (without password) on the master node

\begin{lstlisting}
ssh-keygen -f /home/ceph/.ssh/id_rsa
\end{lstlisting}

Distribute public key to each node

\begin{lstlisting}
cat /home/ceph/.ssh/id_rsa.pub >> /home/ceph/.ssh/authorized_keys
\end{lstlisting}

Copy keys to master node

\begin{lstlisting}
scp /home/ceph/.ssh/id_rsa* root@crvm3:/home/ceph/.ssh/
chown -R ceph:users /home/ceph/
\end{lstlisting}

On admin node initiate cluster deploy

\begin{lstlisting}
su - ceph
mkdir ceph-admin
cd ceph-admin
ceph-deploy new crvm{3,4,5}
\end{lstlisting}

The add the following lines into \code{ceph.conf}

\begin{lstlisting}
cluster_network = 10.94.0.0/16
\end{lstlisting}

\begin{lstlisting}
ceph-deploy install crvm{3,4,5} --no-adjust-repos
ceph-deploy --overwrite-conf mon create-initial
ceph-deploy gatherkeys crvm{3,4,5}
ceph-deploy disk zap crvm{3,4,5}:sdc
ceph-deploy osd create crvm{3,4,5}:sdc
\end{lstlisting}

Side note: if something gone wrong and need to wipe
the complete ceph installation on all nodes do the
following on the admin node

\begin{lstlisting}
su - ceph
cd ceph-admin
ceph-deploy purge crvm{3,4,5}
ceph-deploy purgedata crvm{3,4,5}
ceph-deploy forgetkeys
rm ceph.*
\end{lstlisting}

\section{Setting up Kubernetes}
\label{sec:setup-kuber}

Docker containers are managed by \urlref{https://kubernetes.io/}{Kubernetes},
which requires \urlref{https://github.com/coreos/etcd}{etcd} and
\urlref{https://github.com/coreos/flannel}{flannel} for our setup case.

\subsection{Configure both nodes}
\label{subsec:setup-kuber-both}

First install the programs needed

\begin{lstlisting}
dnf -y install kubernetes flannel
\end{lstlisting}

Edit \code{/etc/kubernetes/config} to assign the
name of the \emph{master} server

\begin{lstlisting}
KUBE_MASTER="--master=http://fed-master:8080"
\end{lstlisting}

Configure flannel by \code{/etc/sysconfig/flanneld}

\begin{lstlisting}
FLANNEL_ETCD_ENDPOINTS="http://fed-master:2379"
FLANNEL_ETCD_PREFIX="/coreos.com/network"
\end{lstlisting}

\subsection{Configure the master node}
\label{subsec:setup-kuber-master}

Install \code{etcd} daemon

\begin{lstlisting}
dnf -y install etcd
\end{lstlisting}

In \code{/etc/etcd/etcd.conf} write

\begin{lstlisting}
ETCD_NAME=default
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
ETCD_ADVERTISE_CLIENT_URLS="http://0.0.0.0:2379"
\end{lstlisting}

Enable and restart it

\begin{lstlisting}
systemctl enable etcd
systemctl restart etcd
\end{lstlisting}

Once the daemon is up and running upload \code{flannel}
configuration by creating \code{flannel-config.json} file

\begin{lstlisting}
{
    "Network": "16.18.0.0/16",
    "SubnetLen": 24,
    "Backend": {
        "Type": "vxlan",
        "VNI": 1
     }
}
\end{lstlisting}

and run

\begin{lstlisting}
etcdctl set /coreos.com/network/config < flannel-config.json
\end{lstlisting}

The enable \code{flannel} itself

\begin{lstlisting}
systemctl enable flanneld
systemctl restart flanneld
\end{lstlisting}

Modify \code{/etc/kubernetes/apiserver}

\label{kube-service-addresses}
\begin{lstlisting}
KUBE_API_ADDRESS="--insecure-bind-address=0.0.0.0"
KUBE_ETCD_SERVERS="--etcd-servers=http://127.0.0.1:2379"
KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=16.18.0.0/16"
KUBE_ADMISSION_CONTROL=""
KUBE_API_ARGS=""
\end{lstlisting}

Update the \code{/etc/kubernetes/kubelet}

\begin{lstlisting}
KUBELET_ADDRESS="--address=0.0.0.0"
KUBELET_HOSTNAME="--hostname-override=fed-master"
KUBELET_API_SERVER="--api-servers=http://fed-master:8080"
KUBELET_ARGS="--cgroup-driver=systemd"
\end{lstlisting}

Restart and enable the services

\begin{lstlisting}
for i in kube-apiserver kube-controller-manager kube-scheduler; do
    systemctl restart $i
    systemctl enable $i
done
\end{lstlisting}

\subsection{Configure the slave node}
\label{subsec:setup-kuber-slave}

Edit \code{/etc/kubernetes/kubelet}

\begin{lstlisting}
KUBELET_ADDRESS="--address=0.0.0.0"
KUBELET_HOSTNAME="--hostname-override=fed-node"
KUBELET_API_SERVER="--api-servers=http://fed-master:8080"
KUBELET_ARGS="--cgroup-driver=systemd"
\end{lstlisting}

Restart and enable the services

\begin{lstlisting}
for i in kube-proxy kubelet flanneld; do
    systemctl restart $i
    systemctl enable $i
done
\end{lstlisting}

Sometimes it is been found that \code{FORWARD} chain on
the \emph{slave} node is having \code{DROP} policy by default
and traffic from outside of the \emph{slave} node is not
passed inside a container. In such case change the default
policy to \code{ACCEPT} on the node boot up procedure.

\begin{lstlisting}
iptables -P FORWARD ACCEPT
\end{lstlisting}

\subsection{Register the slave node}
\label{subsec:setup-kuber-register-slave}

Once both nodes are prepared the \emph{slave} might need
to be registered in kubernetes.

On master node run

\begin{lstlisting}
kubectl get nodes
\end{lstlisting}

If nothing found proceed the following. For this on
\emph{master} node create the following \code{node.json}
file

\begin{lstlisting}
{
    "apiVersion": "v1",
    "kind": "Node",
    "metadata": {
        "name": "fed-node",
        "labels":{ "name": "fed-node-label"}
    },
    "spec": {
        "externalID": "fed-node"
    }
}
\end{lstlisting}

And announce it by
\begin{lstlisting}
kubectl create -f ./node.json
\end{lstlisting}

If everything done right the following report should
appear on the \emph{master} 

\begin{lstlisting}
kubectl get nodes
NAME            STATUS      AGE      VERSION
fed-node        Ready       4h
\end{lstlisting}

\section{Setting up MongoDB}
\label{sec:setup-mongodb}

To track resources \code{swifty} uses
\urlref{https://www.mongodb.com/}{MondoDB} database
running on the \emph{master} node.

\begin{lstlisting}
dnf -y install mongodb-server mongo-tools mongodb
systemctl start mongod
systemctl enable mongod
\end{lstlisting}

Create \emph{swifty} database, add a user and a password,
and prepare collections needed.

A convenient way for this is to use \emph{mongo} interactive shell.

\begin{lstlisting}
use swifty
db.createUser({user:"swifty", pwd:"1q2w3e", roles:[{role:"dbOwner",db:"swifty"}]})
db.createCollection("Function")
db.createCollection("Mware")
db.createCollection("Logs")
db.createCollection("FnStats")
db.createCollection("Balancer")
db.createCollection("BalancerRS")
\end{lstlisting}

The password should be changed to something strong rather than
this weak one used as an example.

Also make sure collections "Function" and "Mware" have uniqe indices
on "cookie" fields.

\section{Setting up middleware}
\label{sec:setup-mware}

Currently the following middleware is supported:
\begin{itemize}
\item{\urlref{https://mariadb.com/}{MariaDB} as sql database;}
\item{\urlref{https://www.rabbitmq.com/}{RabbitMQ} as message queue engine;}
\item{\urlref{https://www.postgresql.org/}{PostgreSQL} as sql database;}
\item{\urlref{https://www.mongodb.com/}{MondoDB} as nosql database.}
\end{itemize}

All of them should be installed on the \emph{slave} node or on any
network reacheable node.

Note the passwords below are for the reference and should be changed
and reflected in appropriate \code{.conf} files.

\subsection{MariaDB}
\label{subsec:setup-mware-mariadb}

Setup the components needed

\begin{lstlisting}
dnf -y install mariadb mariadb-server-utils
systemctl start mariadb
systemctl enable mariadb
\end{lstlisting}

Configure access

\begin{lstlisting}
mysqladmin -u root password aiNe1sah9ichu1re
mysql -u root -p
GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'aiNe1sah9ichu1re' WITH GRANT OPTION;
\end{lstlisting}

\subsection{RabbitMQ}
\label{subsec:setup-mware-rabbitmq}

Setup the components needed

\begin{lstlisting}
dnf -y install rabbitmq-server librabbitmq
systemctl start rabbitmq-server
systemctl enable rabbitmq-server
\end{lstlisting}

Configure access

\begin{lstlisting}
systemctl start rabbitmq-server
systemctl enable rabbitmq-server
rabbitmqctl add_user root airooV1Echah4wei
rabbitmqctl set_user_tags root administrator
rabbitmqctl set_permissions -p / root ".*" ".*" ".*"
rabbitmq-plugins enable rabbitmq_management
\end{lstlisting}

\subsection{MondoDB}
\label{subsec:setup-mware-mondodb}

Setup the components needed

\begin{lstlisting}
dnf -y install mongodb-server mongo-tools mongodb
systemctl enable mongod
systemctl start mongod
\end{lstlisting}

Configure MondoDB to listen requests on net interface rather
than local only. For this edit \code{/etc/mongod.conf} and
comment out \code{bindIP} entry and enable \code{authorization}.

\begin{lstlisting}
net:
  port: 27017
  #bindIp: 127.0.0.1,::1
...
security:
  authorization: 'enabled'
\end{lstlisting}

Create administrator to manage databases remotely

\begin{lstlisting}
use admin
db.createUser({user:"root",
	pwd:"aebaep6chofuY3sh",
	roles: [{role: "userAdminAnyDatabase", db: "admin"}]})
\end{lstlisting}

and finally

\begin{lstlisting}
systemctl restart mongod
\end{lstlisting}

\subsection{PostgreSQL}
\label{subsec:setup-mware-postgresql}

Setup the components needed

\begin{lstlisting}
dnf -y install postgresql-server postgresql postgresql-contrib
postgresql-setup --initdb --unit postgresql
systemctl enable postgresql
systemctl start postgresql
\end{lstlisting}

In \code{/var/lib/pgsql/data/postgresql.conf}

\begin{lstlisting}
listen_addresses = '*'
\end{lstlisting}

In \code{/var/lib/pgsql/data/pg\_hba.conf}

\begin{lstlisting}
# TYPE  DATABASE        USER            ADDRESS                 METHOD
local   all             all                                     peer
host    all             all             0.0.0.0/0               md5
host    all             all             ::/0                    md5
\end{lstlisting}

then

\begin{lstlisting}
systemctl restart postgresql
\end{lstlisting}

\section{Setting up \code{swifty} daemon}
\label{sec:setup-swifty}

The \code{swifty} daemon will run on the master node
where Kubernetes is spinning.

\subsection{Source installation}
\label{subsec:setup-swifty-src}

To build \code{swifty} install \urlref{https://golang.org/}{Go}
language at least version
\urlref{https://redirector.gvt1.com/edgedl/go/go1.9.2.linux-amd64.tar.gz}{go-1.9.2}.

\begin{lstlisting}
dnf -y install git make
tar -C /usr/local/bin -xzf go1.9.2.linux-amd64.tar.gz
mkdir -p ~/go
export GOROOT=/usr/local/bin/go
export GOPATH=~/go
export PATH=${PATH}:${GOROOT}/bin:${GOPATH}/bin
\end{lstlisting}

Install dependency packages

\begin{lstlisting}
go get github.com/willf/bitset
go get gopkg.in/yaml.v2
go get github.com/gorilla/mux
go get go.uber.org/zap
go get gopkg.in/mgo.v2
go get github.com/go-sql-driver/mysql
go get github.com/streadway/amqp
go get github.com/michaelklishin/rabbit-hole
go get gopkg.in/robfig/cron.v2
\end{lstlisting}

Bindings for Kubernetes is a bit tricky -- make sure you have v2.0.0 installed,
otherwise the building procedure may fail

\begin{lstlisting}
go get k8s.io/client-go/...
cd ~/go/src/k8s.io/client-go
git checkout -fb v2.0.0  v2.0.0
go get k8s.io/client-go/...
\end{lstlisting}

Then unpack \code{swifty} archive and run

\begin{lstlisting}
make swifty
  GEN      swifty
ln -s ${path-to-sources}/swifty /usr/sbin/swifty
\end{lstlisting}

To run tests install YAML engine for Python

\begin{lstlisting}
dnf -y install python3-PyYAML.x86_64
\end{lstlisting}

\subsection{Binary installation}
\label{subsec:setup-swifty-bin}

To install binary just put it somewhere into \code{/usr/bin}
or \code{/usr/sbin}. FIXME: Web-UI.

\subsection{Configure}
\label{subsec:setup-swifty-conf}

Because \code{swifty} daemon needs to communicate with Kubernetes,
Keystone, MongoDB, Docker and etc, create the \code{swifty.conf.yaml}
configuration file as below

\begin{lstlisting}
---
db:
        name: "swifty"
        address: "127.0.0.1:27017"
        user: "swifty"
        password: "1q2w3e"
keystone:
	address: "192.168.122.68:5000"
	user: "admin"
	password: "Cae6ThiekuShiece"
	domain: "swifty"
daemon:
        address: "192.168.122.68:8686"
        view: "/home/swifty/src/gate/view"
        sources:
                share: "/home/swifty-volume:/swifty-volume"
                clone: "/home/swifty/local-sources"
        loglevel: "debug"
        balancer:
                iprange: "10.8.0.0/24"
                device: "flannel.1"
middleware:
        maria:
                implementation: "mariadb"
                address: "192.168.122.102:3306"
                user: "root"
                password: "aiNe1sah9ichu1re"
        rabbit:
                implementation: "rabbitmq"
                address: "192.168.122.102:5672"
                user: "root"
                password: "airooV1Echah4wei"
                admport: "15672"
runtime:
        images:
                golang: "swifty/golang"
                python: "swifty/python"
wdog:
        ct-path: "/usr/bin/swy-wdog"
        address: ":8687"
kubernetes:
        config-path: src/conf/kubeconfig
\end{lstlisting}

where
\begin{itemize}
\item{\yamlid{db} describes connection to MongoDB;}
\item{\yamlid{daemon} describes \code{swifty} connection;}
\item{\yamlid{middleware} describes connetions to middleware;}
\item{\yamlid{runtime} describes mapping for Docker images;}
\item{\yamlid{wdog} describes \code{swy-wdog} watchog;}
\item{\yamlid{kubernetes} contains path to config for Kubernetes settings.}
\end{itemize}

Makre sure the IP addresses, login and passwords are valid.

The Kubernetes settings file should be the following

\begin{lstlisting}
Output:
apiVersion: v1
clusters:
- cluster:
    server: http://fed-master:8080
  name: fed-cluster
contexts:
- context:
    cluster: fed-cluster
  name: fed-context
current-context: fed-context
kind: Config
preferences: {}
\end{lstlisting}

When running from \code{swifty} source code, all settings are present
in \code{src/conf} directory.

\section{Setting up \code{swy-s3} daemon}
\label{sec:setup-swy-s3}

Build \code{swy-s3}

\begin{lstlisting}
dnf -y install gcc librados2-devel librbd-devel
go get "github.com/ceph/go-ceph/rados"
make swy-s3
\end{lstlisting}

Provide config file \code{conf/s3.yaml}

\begin{lstlisting}
---
db:
        state: "swifty-s3"
        address: "127.0.0.1:27017"
        user: "swifty"
        password: "MGOS3PASS"
daemon:
        address: "127.0.0.1:8787"
        loglevel: "debug"
ceph:
        config-path: src/ceph.conf
\end{lstlisting}

Prepare backend database

\begin{lstlisting}
use swifty-s3
db.createUser({user:"swifty", pwd:"1q2w3e", roles:[{role:"dbOwner",db:"swifty"}]})
db.createCollection("S3Buckets")
db.createCollection("S3Objects")
db.createCollection("S3ObjectData")
db.createCollection("S3AccessKeys")
db.S3AccessKeys.insert({ "_id" : ObjectId("5a16ccdbb3e8ee4bdf83da35"),
	"status" : 1,
	"access-key-id" : "6DLA43X797XL2I42IJ33",
	"access-key-secret" : "AJwz9vZpdnz6T5TqEDQOEFos6wxxCnW0qwLQeDcB",
	"kind" : 1 })
\end{lstlisting}

Run test (without Ceph rados support and without secrets)

\begin{lstlisting}
swy-s3 --conf conf/s3.yaml --no-rados --no-secrets --db-pass "1q2w3e"
\end{lstlisting}

and in another terminal

\begin{lstlisting}
python3 test/s3.py
\end{lstlisting}

\section{Create Docker images}
\label{sec:setup-images}

Since \code{swifty} is in pre-alfa stage the images are not public
accessible yet and should be build from \code{swifty} source code.

Copy source code archine into the \emph{slave} node (previously
installing Go language on the \emph{slave} node, the same way
as in \ref{subsec:setup-swifty-src}). Then install dependency packages
and initiate image building procedure

\begin{lstlisting}
go get gopkg.in/yaml.v2
go get go.uber.org/zap
make images
\end{lstlisting}

If everything passed fine list the images built

\begin{lstlisting}
docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
swifty/python       latest              86b201a7fad6        6 seconds ago       745 MB
...
\end{lstlisting}

Images with \code{swifty/} prefix are ones built for \code{swifty} usage.
